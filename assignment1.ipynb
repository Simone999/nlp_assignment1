{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simone999/nlp_assignment1/blob/main/assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s11XVB5v9JzV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Callable, Dict, Iterable\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras as ks\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/My Drive/dependency_treebank\" \"dependency_treebank\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yi5UK4UNI6n",
        "outputId": "f54328f1-a791-42a0-b21d-5aa99cd0093c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "structuring dataframe"
      ],
      "metadata": {
        "id": "PGVohBYSOmcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "dataset_path = os.path.join(os.getcwd(), dataset_name)\n",
        "end_train = 100\n",
        "end_validation = 150\n",
        "end_test = 199\n",
        "\n",
        "def create_dataset(start, end, split:str):\n",
        "  tagged_sentences = []\n",
        "  for data_file in range(start, end+1):\n",
        "    filename = os.path.join(dataset_path, \"wsj_%04d.dp\" % data_file)\n",
        "    with open(filename, mode='r', encoding='utf-8') as text_file:  \n",
        "      corpus = text_file.read()\n",
        "      tagged_sentences += corpus.split(\"\\n\\n\")\n",
        "\n",
        "  X = [] # store input sequence\n",
        "  Y = [] # store output sequence\n",
        "  for sentence in tqdm(tagged_sentences):\n",
        "      X_sentence = []\n",
        "      Y_sentence = []\n",
        "\n",
        "      for tagged_word in sentence.rstrip('\\n').split(\"\\n\"):       \n",
        "          entity = tagged_word.split(\"\\t\")\n",
        "          X_sentence.append(entity[0])  # entity[0] contains the word\n",
        "          Y_sentence.append(entity[1])  # entity[1] contains corresponding tag          \n",
        "      X.append(X_sentence)\n",
        "      Y.append(Y_sentence)\n",
        "\n",
        "  assert len(tagged_sentences) == len(X)\n",
        "\n",
        "  df = pd.DataFrame({'sentence':X, 'labels':Y})\n",
        "  df['split'] = split\n",
        "  return df\n",
        "\n",
        "train_set = create_dataset(1, end_train, 'train')\n",
        "val_set = create_dataset(end_train+1, end_validation, 'validation')\n",
        "test_set = create_dataset(end_validation, end_test, 'test')\n",
        "dataset = pd.concat([train_set, val_set, test_set])\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "91ID5ISrBFIW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "64793e49-0094-4647-c8cd-bd1351df9a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1963/1963 [00:00<00:00, 66811.26it/s]\n",
            "100%|██████████| 1299/1299 [00:00<00:00, 57459.85it/s]\n",
            "100%|██████████| 661/661 [00:00<00:00, 40034.58it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sentence  \\\n",
              "0    [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
              "1    [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
              "2    [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
              "3    [A, form, of, asbestos, once, used, to, make, ...   \n",
              "4    [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
              "..                                                 ...   \n",
              "656  [They, also, said, that, more, than, a, dozen,...   \n",
              "657  [Sen., Kennedy, said, in, a, separate, stateme...   \n",
              "658  [Trinity, Industries, Inc., said, it, reached,...   \n",
              "659                   [Terms, were, n't, disclosed, .]   \n",
              "660  [Trinity, said, it, plans, to, begin, delivery...   \n",
              "\n",
              "                                                labels  split  \n",
              "0    [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  train  \n",
              "1    [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  train  \n",
              "2    [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  train  \n",
              "3    [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  train  \n",
              "4    [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  train  \n",
              "..                                                 ...    ...  \n",
              "656  [PRP, RB, VBD, IN, JJR, IN, DT, NN, NNS, VBP, ...   test  \n",
              "657  [NNP, NNP, VBD, IN, DT, JJ, NN, IN, PRP, VBZ, ...   test  \n",
              "658  [NNP, NNPS, NNP, VBD, PRP, VBD, DT, JJ, NN, TO...   test  \n",
              "659                             [NNS, VBD, RB, VBN, .]   test  \n",
              "660  [NNP, VBD, PRP, VBZ, TO, VB, NN, IN, DT, JJ, N...   test  \n",
              "\n",
              "[3923 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c7b58e0-8561-4100-b1d3-7797cf1f76ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
              "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
              "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
              "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>[They, also, said, that, more, than, a, dozen,...</td>\n",
              "      <td>[PRP, RB, VBD, IN, JJR, IN, DT, NN, NNS, VBP, ...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>[Sen., Kennedy, said, in, a, separate, stateme...</td>\n",
              "      <td>[NNP, NNP, VBD, IN, DT, JJ, NN, IN, PRP, VBZ, ...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>[Trinity, Industries, Inc., said, it, reached,...</td>\n",
              "      <td>[NNP, NNPS, NNP, VBD, PRP, VBD, DT, JJ, NN, TO...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>[Terms, were, n't, disclosed, .]</td>\n",
              "      <td>[NNS, VBD, RB, VBN, .]</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>[Trinity, said, it, plans, to, begin, delivery...</td>\n",
              "      <td>[NNP, VBD, PRP, VBZ, TO, VB, NN, IN, DT, JJ, N...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3923 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c7b58e0-8561-4100-b1d3-7797cf1f76ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c7b58e0-8561-4100-b1d3-7797cf1f76ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c7b58e0-8561-4100-b1d3-7797cf1f76ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(set([word.lower() for sentence in dataset['sentence'] for word in sentence]))\n",
        "num_tags   = len(set([word.lower() for sentence in dataset['labels'] for word in sentence]))\n",
        "\n",
        "print(\"Total number of tagged sentences: {}\".format(len(dataset)))\n",
        "print(\"Vocabulary size: {}\".format(num_words))\n",
        "print(\"Total number of tags: {}\".format(num_tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us9lT6vuQ7lK",
        "outputId": "784f29c4-033f-4a1e-a815-2b1ff21e315d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tagged sentences: 3923\n",
            "Vocabulary size: 10947\n",
            "Total number of tags: 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot label distributions\n",
        "\n",
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# def flatten(arr):\n",
        "#   return [item for sublist in arr for item in sublist]\n",
        "\n",
        "# def plot_labels_distribution(dataset, title: str):\n",
        "#     train_data = flatten((dataset.loc[dataset['split'] == \"train\"])['labels'])\n",
        "#     val_data = flatten((dataset.loc[dataset['split'] == \"validation\"])['labels'])\n",
        "#     test_data = flatten((dataset.loc[dataset['split'] == \"test\"])['labels'])\n",
        "                    \n",
        "#     classes = flatten(dataset['labels'])\n",
        "#     bins = np.linspace(0, len(classes), len(classes) + 1, dtype='int32')\n",
        "#     plt.title(title)\n",
        "#     plt.hist([train_data, val_data, test_data], bins=bins, label=['train', 'val', 'test'])\n",
        "    \n",
        "#     plt.legend(loc='upper right')    \n",
        "    \n",
        "#     x_ticks_names = classes\n",
        "#     x_ticks_pos = [(i + 0.5) for i in np.arange(len(x_ticks_names))]\n",
        "    \n",
        "#     plt.xticks(x_ticks_pos, x_ticks_names, rotation=90)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# plot_labels_distribution(dataset, 'Tags distribution');"
      ],
      "metadata": {
        "id": "ZjDggP7ZV_JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glove embeddings"
      ],
      "metadata": {
        "id": "dMVaMbBDOyJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "def load_embedding_model(model_type: str='glove', embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "    download_path = \"\"\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "        \n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        print('FastText: 300')\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "embedding_dimension=50\n",
        "embedding_model = load_embedding_model(model_type=\"glove\", embedding_dimension=embedding_dimension)"
      ],
      "metadata": {
        "id": "_D-RYb4L_EiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9be98b8-2a70-431b-ee4e-863ec97cea7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words(sentences: List[str]):\n",
        "  return set(token for tokens in sentences for token in tokens)\n",
        "\n",
        "class Tokenizer:\n",
        "  def __init__(self, vocabulary=None):\n",
        "    \"\"\"\n",
        "    Transform a set of sentences in a set of indices.\n",
        "    The order of enumeration respects the order of the set of sentences used in the fit_on_texts method.\n",
        "    \"\"\"\n",
        "    self.word_to_index = {}\n",
        "    self.idx_to_word = {}\n",
        "    self.__idx = 1\n",
        "\n",
        "    if vocabulary:\n",
        "      self.update_vocabulary(vocabulary)\n",
        "\n",
        "  def get_oov_terms(self, words):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    words_in_vocabulary = set(self.word_to_index.keys())\n",
        "    oov = set(words).difference(words_in_vocabulary)\n",
        "    return list(oov)\n",
        "\n",
        "  def update_vocabulary(self, sentences, verbose=True):\n",
        "    \"\"\"\n",
        "    Update the vocabulary by looking at the list of sentences in input\n",
        "    \"\"\"\n",
        "    \n",
        "    words = get_words(sentences) \n",
        "    oov_terms = self.get_oov_terms(words)\n",
        "\n",
        "    old_len = len(self.word_to_index)\n",
        "    self.__expand_vocabulary(oov_terms)\n",
        "\n",
        "    if verbose:\n",
        "      oov_percentage = float(len(oov_terms)) * 100 / len(words)\n",
        "      print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")\n",
        "\n",
        "      print(\"Vocabulary length before expansion:\", old_len)\n",
        "      print(\"Vocabulary length after expansion:\", len(self.word_to_index))\n",
        "\n",
        "    return oov_terms\n",
        "\n",
        "  def __expand_vocabulary(self, oov_terms: Iterable[str]):\n",
        "    for term in oov_terms:\n",
        "      self.word_to_index[term] = self.__idx\n",
        "      self.idx_to_word[self.__idx] = term\n",
        "      self.__idx += 1\n",
        "\n",
        "  def texts_to_sequences(self, sentences):\n",
        "    \"\"\"\n",
        "    Transform a list of sentences in a list of sequences, according to the current vocabulary\n",
        "    \"\"\"\n",
        "    return list(map(self.__sentence_to_sequence, sentences))\n",
        "\n",
        "  def __sentence_to_sequence(self, sentence):\n",
        "    return list(map(lambda token: self.word_to_index[token], sentence))\n",
        "\n",
        "  def sequences_to_texts(self, sequences):\n",
        "    \"\"\"\n",
        "    Transform a list of sequences in a list of sentences, according to the current vocabulary\n",
        "    \"\"\"\n",
        "    return list(map(self.__sequence_to_text, sequences))\n",
        "  \n",
        "  def __sequence_to_text(self, sequence):\n",
        "    return list(map(lambda idx: self.idx_to_word[idx], sequence))\n",
        "\n",
        "class EmbeddingMatrix():\n",
        "  def __init__(self,\n",
        "               embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "               embedding_dimension,\n",
        "               word_to_idx: Dict[str, int],\n",
        "               oov_vector_factory = None) -> None:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param embedding_dimension: dimension of the vectors in the embedding space\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    \"\"\"\n",
        "    self.oov_vector_factory = oov_vector_factory if oov_vector_factory else lambda: np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "    vocab_size = len(word_to_idx)\n",
        "    self.embedding_matrix = np.zeros((vocab_size + 1, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      if word in embedding_model:\n",
        "        embedding_vector = embedding_model[word]\n",
        "      else:\n",
        "        embedding_vector = self.oov_vector_factory()\n",
        "\n",
        "      self.embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "  def update_matrix(self, word_to_idx: Dict[str, int]):\n",
        "    old_vocab_size = len(self.embedding_matrix) - 1\n",
        "    vocab_size = len(word_to_idx)\n",
        "    embedding_dimension = self.embedding_matrix.shape[1]\n",
        "\n",
        "    self.embedding_matrix.resize((vocab_size + 1, embedding_dimension))\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      if idx > old_vocab_size:\n",
        "        self.embedding_matrix[idx] = self.oov_vector_factory()\n",
        "\n",
        "  def as_numpy(self):\n",
        "    return self.embedding_matrix"
      ],
      "metadata": {
        "id": "UcaZn7SlgnLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_oov(dataset, dataset_name: str):\n",
        "  print()\n",
        "  print(f'Adding {dataset_name} vocabulary ...')\n",
        "  tokenizer.update_vocabulary(dataset.sentence)\n",
        "  embedding_matrix.update_matrix(tokenizer.word_to_index)\n",
        "\n",
        "tokenizer = Tokenizer([embedding_model.vocab.keys()])\n",
        "embedding_matrix = EmbeddingMatrix(embedding_model, embedding_dimension, tokenizer.word_to_index)\n",
        "\n",
        "add_oov(train_set, 'training set')\n",
        "add_oov(val_set, 'validation set')\n",
        "add_oov(test_set, 'test set')\n",
        "\n",
        "print('\\n')\n",
        "print('Shape of the embedding matrix', embedding_matrix.as_numpy().shape)"
      ],
      "metadata": {
        "id": "DN-yZO_1XcX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b37f6c-b612-4555-92ef-a92be293b781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 400000 (100.00%)\n",
            "Vocabulary length before expansion: 0\n",
            "Vocabulary length after expansion: 400000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400000/400000 [00:01<00:00, 290261.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adding training set vocabulary ...\n",
            "Total OOV terms: 2346 (29.29%)\n",
            "Vocabulary length before expansion: 400000\n",
            "Vocabulary length after expansion: 402346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 402346/402346 [00:00<00:00, 2034650.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adding validation set vocabulary ...\n",
            "Total OOV terms: 944 (16.02%)\n",
            "Vocabulary length before expansion: 402346\n",
            "Vocabulary length after expansion: 403290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 403290/403290 [00:00<00:00, 2112721.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adding test set vocabulary ...\n",
            "Total OOV terms: 455 (12.49%)\n",
            "Vocabulary length before expansion: 403290\n",
            "Vocabulary length after expansion: 403745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 403745/403745 [00:00<00:00, 2023759.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Shape of the embedding matrix (403746, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.as_numpy().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFY_nkBEbUHY",
        "outputId": "fdbd2263-1f3c-476f-dc0c-f8b72656e80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(403746, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(dataset: pd.DataFrame, max_seq_length):\n",
        "  encoded_sentences = tokenizer.texts_to_sequences(dataset.sentence)\n",
        "  encoded_sentences = ks.utils.pad_sequences(encoded_sentences, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\")\n",
        "\n",
        "  encoded_labels = label_tokenizer.texts_to_sequences(dataset.labels)\n",
        "  encoded_labels = ks.utils.pad_sequences(encoded_labels, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\")\n",
        "\n",
        "  return (\n",
        "      tf.data.Dataset.from_tensor_slices((encoded_sentences, encoded_labels))\n",
        "      # .cache()\n",
        "      # .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "      )\n",
        "\n",
        "max_seq_length = 1000\n",
        "\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.update_vocabulary(dataset.labels, verbose=False)\n",
        "\n",
        "train_ds = create_dataloader(train_set, max_seq_length)\n",
        "val_ds = create_dataloader(val_set, max_seq_length)"
      ],
      "metadata": {
        "id": "pKV6c2DFbiAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text, labels = next(train_ds.take(1).as_numpy_iterator())\n",
        "# print(text)\n",
        "# print(labels)\n",
        "# print(len(train_set.sentence[0]))"
      ],
      "metadata": {
        "id": "VlW0ttgairLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network baseline"
      ],
      "metadata": {
        "id": "DF4pIZ2kjyFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(embedding_weights):\n",
        "  inputs = ks.layers.Input(shape=(None, ))\n",
        "  x = ks.layers.Embedding(*embedding_weights.shape, weights=[embedding_weights])(inputs)\n",
        "  rnn = ks.layers.LSTM(units=64, return_sequences=True)\n",
        "  x = ks.layers.Bidirectional(rnn)(x)\n",
        "  x = ks.layers.TimeDistributed(ks.layers.Dense(45, activation='softmax'))(x)\n",
        "\n",
        "  return ks.models.Model(inputs, x)\n",
        "\n",
        "\n",
        "embedding_weights = embedding_matrix.as_numpy()\n",
        "model = create_model(embedding_weights)\n",
        "model.summary()\n",
        "\n",
        "# #Create Architecture\n",
        "# lstm_model = Sequential()\n",
        "# # vocabulary size — number of unique words in data\n",
        "# # length of vector with which each word is represented\n",
        "# lstm_model.add(Embedding(input_dim = len(vocabulary), \n",
        "# output_dim = 50, \n",
        "# # length of input sequence\n",
        "# input_length = 100, \n",
        "# # word embedding matrix\n",
        "# weights = [embedding_matrix],\n",
        "# # True — update embeddings_weight matrix\n",
        "# trainable = True \n",
        "# ))\n",
        "# # add an LSTM layer which contains 64 LSTM cells\n",
        "# # True — return whole sequence; False — return single output of the end of the sequence\n",
        "# lstm_model.add(LSTM(64, return_sequences=True))\n",
        "# lstm_model.add(TimeDistributed(Dense(num_tags, activation='softmax')))\n",
        "# #compile model\n",
        "# lstm_model.compile(loss      =  'categorical_crossentropy',\n",
        "#                   optimizer =  'adam',\n",
        "#                   metrics   =  ['acc'])\n",
        "# # check summary of the model\n",
        "# lstm_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD806mIWs0zT",
        "outputId": "08641790-0217-4559-e80a-c7892a78f819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 50)          20187300  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 128)        58880     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, None, 45)         5805      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,251,985\n",
            "Trainable params: 20,251,985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "metadata": {
        "id": "n9vN6BpspXn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(train_set.sentence.values)\n",
        "sequences[:10]"
      ],
      "metadata": {
        "id": "WixRgU7BnVOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487267f7-bda1-4e6f-9d8f-b76dd7caf8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[401341,\n",
              "  400191,\n",
              "  258712,\n",
              "  238658,\n",
              "  55202,\n",
              "  72706,\n",
              "  258712,\n",
              "  381093,\n",
              "  350838,\n",
              "  66392,\n",
              "  184243,\n",
              "  225554,\n",
              "  101864,\n",
              "  134325,\n",
              "  385117,\n",
              "  402163,\n",
              "  184888,\n",
              "  112474],\n",
              " [400974,\n",
              "  400191,\n",
              "  324081,\n",
              "  133587,\n",
              "  188675,\n",
              "  400284,\n",
              "  400405,\n",
              "  258712,\n",
              "  66392,\n",
              "  401196,\n",
              "  264039,\n",
              "  141565,\n",
              "  112474],\n",
              " [401986,\n",
              "  402053,\n",
              "  258712,\n",
              "  61636,\n",
              "  55202,\n",
              "  72706,\n",
              "  278139,\n",
              "  26824,\n",
              "  133587,\n",
              "  188675,\n",
              "  401215,\n",
              "  401359,\n",
              "  400982,\n",
              "  401037,\n",
              "  258712,\n",
              "  381361,\n",
              "  204458,\n",
              "  101864,\n",
              "  134325,\n",
              "  385117,\n",
              "  188675,\n",
              "  236465,\n",
              "  402314,\n",
              "  216114,\n",
              "  20603,\n",
              "  112474],\n",
              " [401540,\n",
              "  247717,\n",
              "  188675,\n",
              "  132277,\n",
              "  70566,\n",
              "  67760,\n",
              "  355336,\n",
              "  1612,\n",
              "  400849,\n",
              "  91431,\n",
              "  294753,\n",
              "  51551,\n",
              "  360728,\n",
              "  101864,\n",
              "  369443,\n",
              "  223173,\n",
              "  188675,\n",
              "  60030,\n",
              "  236271,\n",
              "  35090,\n",
              "  101864,\n",
              "  141565,\n",
              "  188675,\n",
              "  288564,\n",
              "  333655,\n",
              "  355336,\n",
              "  210782,\n",
              "  280513,\n",
              "  109699,\n",
              "  222122,\n",
              "  55202,\n",
              "  48553,\n",
              "  258712,\n",
              "  241009,\n",
              "  87185,\n",
              "  112474],\n",
              " [401620,\n",
              "  132277,\n",
              "  115382,\n",
              "  258712,\n",
              "  400736,\n",
              "  258712,\n",
              "  324081,\n",
              "  369554,\n",
              "  192732,\n",
              "  70566,\n",
              "  210782,\n",
              "  170191,\n",
              "  66392,\n",
              "  184959,\n",
              "  258712,\n",
              "  44832,\n",
              "  34691,\n",
              "  118273,\n",
              "  8409,\n",
              "  355336,\n",
              "  210782,\n",
              "  356130,\n",
              "  249531,\n",
              "  180092,\n",
              "  144550,\n",
              "  218659,\n",
              "  331203,\n",
              "  44364,\n",
              "  258712,\n",
              "  241009,\n",
              "  110797,\n",
              "  112474],\n",
              " [402295,\n",
              "  401634,\n",
              "  258712,\n",
              "  66392,\n",
              "  174683,\n",
              "  188675,\n",
              "  401848,\n",
              "  401468,\n",
              "  402189,\n",
              "  402113,\n",
              "  180092,\n",
              "  205575,\n",
              "  400849,\n",
              "  393614,\n",
              "  258712,\n",
              "  188142,\n",
              "  265769,\n",
              "  400736,\n",
              "  393874,\n",
              "  217188,\n",
              "  402087,\n",
              "  91431,\n",
              "  294753,\n",
              "  393874,\n",
              "  80018,\n",
              "  112474],\n",
              " [400379,\n",
              "  135857,\n",
              "  329115,\n",
              "  202246,\n",
              "  87185,\n",
              "  280513,\n",
              "  109699,\n",
              "  101864,\n",
              "  324106,\n",
              "  48553,\n",
              "  258712,\n",
              "  66392,\n",
              "  176559,\n",
              "  257996,\n",
              "  182044,\n",
              "  393874,\n",
              "  177902,\n",
              "  112247,\n",
              "  401848,\n",
              "  401076,\n",
              "  401924,\n",
              "  188675,\n",
              "  401268,\n",
              "  258712,\n",
              "  101864,\n",
              "  302464,\n",
              "  135571,\n",
              "  355336,\n",
              "  66035,\n",
              "  135699,\n",
              "  265359,\n",
              "  355336,\n",
              "  66392,\n",
              "  388438,\n",
              "  112474],\n",
              " [401540,\n",
              "  402295,\n",
              "  93629,\n",
              "  110797,\n",
              "  258712,\n",
              "  355363,\n",
              "  400331,\n",
              "  324081,\n",
              "  274537,\n",
              "  72706,\n",
              "  209746,\n",
              "  112474],\n",
              " [401961,\n",
              "  89485,\n",
              "  333371,\n",
              "  248094,\n",
              "  55202,\n",
              "  48553,\n",
              "  244798,\n",
              "  277432,\n",
              "  110436,\n",
              "  188675,\n",
              "  132277,\n",
              "  385106,\n",
              "  251226,\n",
              "  268655,\n",
              "  151025,\n",
              "  112474],\n",
              " [400132, 324081, 260362, 132277, 393874, 6724, 95787, 96014, 112474, 157562]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_training = lstm_model.fit(embedding_matrix, y_train, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "3mI-znnbxYI9",
        "outputId": "c8a54c5b-a53e-4647-e766-36745d826374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-162e2706e922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type builtin_function_or_method)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file = os.path.join(os.getcwd(),\"Glove\", \"glove.6B.50d.txt\")\n",
        "\n",
        "print (\"Loading Glove Model\")\n",
        "with open(glove_file, encoding=\"utf8\" ) as f:\n",
        "    lines = f.readlines()\n",
        "vocabulary = {}\n",
        "for line in lines:\n",
        "    splits = line.split()\n",
        "    vocabulary[splits[0]] = np.array([float(val) for val in splits[1:]])\n",
        "print (\"Done.\",len(vocabulary.keys()),\" words loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S96Qx5WTSDIZ",
        "outputId": "5b8cb147-f640-42f8-8d50-550e32af592b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Glove Model\n",
            "Done. 400000  words loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def findembedding(word):\n",
        "    if word in vocabulary.keys():\n",
        "        embedding = vocabulary[word]\n",
        "    else:\n",
        "        embedding = [0]*50\n",
        "    return embedding\n",
        "\n",
        "def glovesent(sentence):\n",
        "    matrix = [findembedding(word) for word in tokenizer.tokenize(str(sentence))]\n",
        "    matrix = np.array(matrix)\n",
        "    return np.average(matrix, axis=0)\n",
        "\n",
        "\n",
        "glove_X_train = np.array([glovesent(sentence) for sentence in train_set])\n",
        "glove_X_test = np.array([glovesent(sentence) for sentence in test_set])\n",
        "\n",
        "print(glove_X_train.shape)\n"
      ],
      "metadata": {
        "id": "Q92Ni_hCaWay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}